#+hugo_base_dir: /home/guilherme/Projects/blog/hugo-blog/
* blog
:PROPERTIES:
:EXPORT_HUGO_SECTION: blog
:END:
** DONE Using org mode and ox-hugo to replace markdown in hugo workflow
CLOSED: [2019-01-13 dom 13:41]
:PROPERTIES:
:EXPORT_FILE_NAME: 2019-01-01-using-org-mode-and-ox-hugo-to-replace-markdown-in-hugo-workflow
:END:

I have decided to give org mode blogging a go. Why org mode? The main reasons for this were:

- Abandon markdown: I always get confused by markdown markup choices. I find myself constantly reaching for [[https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet][markdown cheatsheet]] to find out how to insert a link. "Is it parenthesis or brackets?" gets me every time;
- Increase familiarity with org mode synthax to use it in my literary programming workflows in the near future (tangle+babel);
- Reduce friction to create new blog posts using the great [[https://ox-hugo.scripter.co/][ox-hugo]] by Kaushal Modi.

For anyone trying to do the same, I recommend:

- [[https://www.kengrimes.com/ox-hugo-tutorial/][ox-hugo tutorial by Ken Grimes]]
- [[https://ox-hugo.scripter.co/][ox-hugo website/manual]]
- [[https://karl-voit.at/2017/09/23/orgmode-as-markup-only/][Some org mode markup notes and comments by Karl Voigt]]
- [[http://ergoemacs.org/emacs/emacs_org_markup.html][Most frequently used org mode markup by Xah]]

Since I want this post to be self contained for further reference in the near future, I will summarise what I've learned from the references above.

*** Installation and .emacs setup

#+begin_quote
*EDIT Apr 04 2020:* Instructions updated after noticing they failed in brand new system installation
#+end_quote

- Installation with package manager:
#+begin_src emacs-lisp
  M-x package-install RET ox-hugo RET
#+end_src
- Require it in the .emacs file:
#+begin_src org
  (with-eval-after-load 'ox__
    (require 'ox-hugo))
#+end_src
- To take advantage of auto exporting on save I added the following to my .emacs file:
#+begin_src org
  ;; Hugo orgmode exporter
  (require 'org-hugo-auto-export-mode) ;If you want the auto-exporting on file saves
#+end_src
- Enable snippets shortcuts
#+begin_src org
  (require 'org-tempo);Enable snippets expantions (ex: <s+TAB or <q+TAB)
#+end_src
- Now to create a capture template to create new blog postos on the fly:
#+begin_src org
  ;; Populates only the EXPORT_FILE_NAME property in the inserted headline.
  (with-eval-after-load 'org-capture
    (defun org-hugo-new-subtree-post-capture-template ()
      "Returns `org-capture' template string for new Hugo post.
  See `org-capture-templates' for more information."
      (let* ((title (read-from-minibuffer "Post Title: ")) ;Prompt to enter the post title
             (fname (org-hugo-slug title)))
        (mapconcat #'identity
                   `(
                     ,(concat "* TODO " title)
                     ":PROPERTIES:"
                     ,(concat ":EXPORT_FILE_NAME: " (format-time-string "%Y-%m-%d-") fname)
                     ":END:"
                     "%?\n")          ;Place the cursor here finally
                   "\n"))))

  ;; org capture templates
  (setq org-capture-templates
   '(
     ("h"                ;`org-capture' binding + h
                      "Hugo post"
                      entry
                      ;; It is assumed that below file is present in `org-directory'
                      ;; and that it has a "Blog Ideas" heading. It can even be a
                      ;; symlink pointing to the actual location of all-posts.org!
                      (file+olp "/home/guilherme/blog/content-org/posts.org" "blog")
                      (function org-hugo-new-subtree-post-capture-template))
  ))
#+end_src
- Include a /.dir-locals.el/ file in the project root, assuming all org-files are in a /content-org/ directory below root:
#+begin_src org
  (("content-org/"
    . ((org-mode . ((eval . (org-hugo-auto-export-mode)))))))
#+end_src

*Note:* everything here so far is in the manual. I only added the current date to the file name being created in the ~EXPORT_FILE_NAME:~ property to be consistent with my previous naming scheme.

*** Org file structure

Ken grimes did a great job explaining how to use one org file to organize a hugo blog. I'll just mention a few things. First of all, Hugo has a contents folder and depending on the theme you use (I use cocoa) it will have 

#+begin_src shell :exports both :results output
  tree -d -L 2 ../content
#+end_src

#+results: 
: ../content
: ├── about
: ├── blog
: └── projects
: 
: 3 directories



I haven't been using ox-hugo in my previous posts, so I already have markdown files that do not have a corresponding org version. However, my posts reside in the blog folder. As an example, a minimal org file used to generate this post would be the following:

#+begin_src org
  ,#+hugo_base_dir: /home/guilherme/blog/
   ,* blog
  :PROPERTIES:
  :EXPORT_HUGO_SECTION: blog
  :END:
   ,** TODO Using org mode and ox-hugo to replace markdown in hugo workflow
  :PROPERTIES:
  :EXPORT_FILE_NAME: 2019-01-01-using-org-mode-and-ox-hugo-to-replace-markdown-in-hugo-workflow
  :END:

   ,* Footnotes
   ,* COMMENT Local Variables                          :ARCHIVE:
   # Local Variables:
   # org-hugo-auto-export-on-save: t
   # End:
#+end_src

The local variable ~org-hugo-auto-export-on-save~ with the ARCHIVE tag enables hugo auto export to my blog's master org file only.

*** Workflow

To create a new blog post I simply issue ~C+c C+c + h~ and a title for my new post is prompted. Along with it the filename with date are already set by the property in the capture template. Form now on I just write. 

To view any changes on my working post:

#+begin_src bash
  hugo serve -D --navigateToChanged
#+end_src

When done with the post, just changing the task from TODO to DONE will create a special property date that will be the post's date published.

*** Markup examples
I've put together some markup examples that were spread through the sources mentioned in the beggining of the post. These are for quick lookups, where I can find the synthax for features I use the most.

#+caption: Same Org Logo
#+name: img__org_logo
[[file:/home/gtpedrosa/Projects/blog/hugo-blog/static/img/gnu-unicorn.png]]
*Here we refer to [[img__org_logo]].*

#+begin_quote
Everything should be made as simple as possible,
but not any simpler -- Albert Einstein
#+end_quote

#+begin_src org
  ,* This Is A Heading
  ,** This Is A Sub-Heading
  ,*** And A Sub-Sub-Heading

  Paragraphs are separated by at least one empty line.

  ,*bold* /italic/ _underlined_ +strikethrough+ =monospaced= ~code~
  [[http://Karl-Voit.at][Link description]]
  http://Karl-Voit.at → link without description

  : Simple pre-formatted text such as for source code.
  : This also respects the line breaks. *bold* is not bold here.

  - list item
  - another item
    - sub-item
      1. also enumerated
      2. if you like
  - [ ] yet to be done
  - [X] item which is done	
#+end_src

#+caption: Hello
#+name: code__hello
#+begin_src emacs-lisp
  (message "Hello")
#+end_src
*Here we refer to [[code__helloagain]].*

#+include: "./common.org::#lorem-ipsum" :only-contents t

#+caption: Hello Again
#+name: code__helloagain
#+begin_src emacs-lisp
  (message "Hello again")
#+end_src
*Here we refer to [[code__hello]].*
** DONE Generating documentation of Matlab scripts automatically with mkdocs
CLOSED: [2019-01-20 dom 23:06]
:PROPERTIES:
:EXPORT_FILE_NAME: 2019-01-20-generating-documentation-of-matlab-scripts-automatically-with-mkdocs
:END:

Something that always called my attention is how much more documented and even appealing to newcomers projects in python or R are if compared to Matlab in general. One exception is [[https://github.com/vlfeat/matconvnet][MatConvNet]], a library that implements convolutional neural networks (CNNs) in Matlab and has a good looking and helpful documentation. 

The documentation stands out as it is produced with [[https://www.mkdocs.org/][MkDocs]], a static site generator based on markdown specific for project documentation. Something very different from Matlab's default html help template. But why bother adding another layer of complexity writing markdown files for the documentation? Why not use the source code itself? The authors of MatConvNet created a parser capable of extracting the comments of the functions in mfiles and auto generate the markdown necessary for MkDocs. The python scripts ~matdoc.py~ and ~matdocparser.py~ do exactly that. I must acknowledge it is copywrited material (Copyright (c) 2014-16 The MatConvNet Team) and not made by me.

For the remaining of this post consider a simple project composed of two mfiles: ~addme.m~ and ~subtractme.m~ which do exactly what you would expect from them, add and subtract two numbers. Also consider a ~index.md~ and a ~mkdocs.yml~ file, the bare minimum for a MkDocs documentation. This is the files setup and they can be downloaded from my github [[https://github.com/gtpedrosa/matlab-mkdocs][here]].

#+begin_src shell :exports both :results output
  tree -L 2 ~/Sandbox/docs
#+end_src

#+results: 
#+begin_example
  /home/guilherme/Sandbox/docs
  ├── addme.m
  ├── COPYING
  ├── COPYING~
  ├── docs
  │   └── index.md
  ├── makefile
  ├── matdocparser.py
  ├── matdocparser.pyc
  ├── matdoc.py
  ├── mkdocs.yml
  └── subtractme.m

  1 directory, 10 files
#+end_example

The following make file is responsible for the automation of the pocess:

#+begin_src shell
  PYTHON = python2
  MKDOCS = mkdocs
  SRC = $(wildcard *.m)
  TAR = $(SRC:.m=.md)

  mfiledir = docs/mfiles

  $(info SRC is $(SRC))
  $(info TAR is $(TAR))
  $(info mfiledir is $(mfiledir))

  .PHONY: all clean

  all: $(TAR)

  %.md: %.m matdoc.py matdocparser.py
        $(info $(@))
        $(info $(@D))
        mkdir -p $(mfiledir)
        $(PYTHON) ./matdoc.py "$(<)" > "$(mfiledir)/$(@)"

  doc-serve: mkdocs.yml
        $(MKDOCS) serve

  clean:
        rm -f $(TAR)
        rm -rf $(mfiledir)
#+end_src

Issuing 

#+begin_src shell
  make all
  make doc-serve
#+end_src

This is the expected output:

#+results: 
#+begin_example
  SRC is subtractme.m addme.m
  TAR is subtractme.md addme.md
  mfiledir is docs/mfiles
  subtractme.md
  .
  mkdir -p docs/mfiles
  python2 ./matdoc.py "subtractme.m" > "docs/mfiles/subtractme.md"
  addme.md
  .
  mkdir -p docs/mfiles
  python2 ./matdoc.py "addme.m" > "docs/mfiles/addme.md"
  SRC is subtractme.m addme.m
  TAR is subtractme.md addme.md
  mfiledir is docs/mfiles

  INFO    -  Building documentation... 
  INFO    -  Cleaning site directory 
  [I 190120 22:44:13 server:283] Serving on http://127.0.0.1:8000
  [I 190120 22:44:13 handlers:60] Start watching changes
  [I 190120 22:44:13 handlers:62] Start detecting changes
  [I 190120 22:44:55 handlers:133] Browser Connected: http://localhost:8000/#project-layout
  [I 190120 22:44:55 handlers:133] Browser Connected: http://127.0.0.1:8000/
#+end_example

And finally, the end result, a home page based on the ~index.md~ file:

#+caption: MkDocs documentation generated automatically for a matlab project
#+name: mkdocsproject
[[file:/home/guilherme/blog/static/img/mkdocshome.png]]

And more importantly, automatic documentation for each function in a specific menu called ~M-files~:

#+caption: Documentation for each mfile
#+name: mkdocsfunctions
[[file:/home/guilherme/blog/static/img/mkdocsfunction.png]]

** DONE Using httr with JSON API's                                   :R:APIs:
CLOSED: [2019-03-24 dom 11:35]
:PROPERTIES:
:EXPORT_FILE_NAME: 2019-03-24-using-httr-with-json-api-s
:END:

It is incredbly easy to make R interact with external API's. I've found [[https://cran.r-project.org/web/packages/httr/vignettes/api-packages.html][the httr vignette]] to be a great resource on how to make a wrapper to interact with them. However I've stumbled in one quirk that is worth mentioning.

While creating a body to a POST request message, encoded with json, I verified that my requests were being turned down by the server. The issue was related  to a behavior of the ~jsonlite~ package, one of httr's dependencies which uses  ~auto_unbox=TRUE~ by default. This isn't an issue /per se/ but for length one vector jsonlite returned:

#+begin_src R :exports both :results output
  cat(jsonlite:::toJSON(list(message = "my string"),auto_unbox=T))
#+end_src

#+results: 
: {"message":"my string"}



Whereas the API requested a *boxed* response from length one vectors, such the one you get without the ~auto_unbox=TRUE~ option

#+begin_src R :exports both :results output
  cat(jsonlite:::toJSON(list(message = "my string")))
#+end_src

#+results: 
: {"message":["my string"]}



Reading [[https://github.com/r-lib/httr/issues/159][this github issue]] might give a more in depth explanation.

This led me to rabbit holes such as forking ~httr~ and recompiling with ~auto_unbox=FALSE~ option, which I did. But not without breaking other requests which truly needed to be unboxed.

The solution was simpler than I thought and makes use of the function ~AsIs~ from the base package. It can be called with the ~I(x)~ synthax and changes the class of an object indicating it should be treated /as is/. What this does is to prevent the ~auto_unbox~ behavior on certain fields where this is undesirable, such as in the following example: 

#+begin_src R :exports both :results output
  cat(jsonlite:::toJSON(list(message = "my string",mymessage = I("My other string")),auto_unbox=T))
#+end_src

#+results: 
: {"message":"my string","mymessage":["My other string"]}



This approach not only did not break anything but also made my requests compatible with the server boxed length one vectors specification as required.
** DONE Using SQLAlchemy to navigate an existing database :python:ORM:Databases:
CLOSED: [2019-04-06 sáb 18:56]
:PROPERTIES:
:EXPORT_FILE_NAME: 2019-04-06-using-sqlalchemy-to-navigate-an-existing-database
:END:

Given the task to interact with an existing database I felt compelled to use the ORM abstraction instead of making queries with raw sql. My aim was to avoid the common pitfalls regarding making text templates for sqlqueries, prone to sql injection exploits, and enhance query composability. 

I've found there are essentialy two ways to approach this task: through reflection or a declarative model. Both approaches are explained in the following sections.

*** SQLAlchemy reflection

Reflection uses metadata property to access schema constructs. It offers a few methods to access table objects, which do not have to be explicitly declared. 

The downside of this approach is mantainability, since schema changes can make code unreliable. Here's an eample of how to access the /TimeStamp/ column of a /Table_I_Want_to_Interact/ in a generic database:

#+begin_src python
  from sqlalchemy.orm import sessionmaker
  from sqlalchemy import create_engine, MetaData, Table

  # Using SQLAlchemy reflection example
  engine = create_engine('connectionstringhere')
  table1meta = MetaData(engine)
  table1 = Table('Table_I_Want_to_Interact', table1meta, autoload=True)
  DBSession = sessionmaker(bind=engine)
  session = DBSession()
  results = session.query(table1).filter(table1.columns.TimeStamp>="2019-02-26 18:00:00.000")
  results.all()
#+end_src

*** SQLAlchemy declarative model

The declarative model needs Table objects to be explicitly declared. Due to this inherent verbose nature, I have found it is easier to grasp what is happening and even how the database is structured after a glance at the source code, such as in the following snippet:

#+begin_src python
  from sqlalchemy import create_engine, MetaData, BigInteger, CHAR, Column, DateTime, Float, Integer, SmallInteger, String, Table, Unicode, text
  from sqlalchemy.ext.declarative import declarative_base
  from sqlalchemy.orm import sessionmaker

  Base = declarative_base()
  metadata = Base.metadata


  my_table_object = Table(
      'table_name', metadata,
      Column('Column1', Integer, nullable=False),
      Column('TimeStamp', DateTime, nullable=False),
      Column('Column3', Integer, nullable=False),
      Column('Column4', Unicode(2000))
  )
#+end_src

Here a table named /table_name/ in the database is being mapped to the /my_table_object/ instance. It should be noted that not all columns need to be mapped. Uninteresting columns can be left out with no drawbacks.

Depending on the database structure size, however,  it could be cumbersome to define multiple tables. For use cases like this, I have found the package [[https://pypi.org/project/sqlacodegen/][sqlacodegen]] of great help. It automates the task of creating the declarative models for you. Providing an output file and a connections tring it is as easy as issuing:

#+begin_src shell
  sqlcodegen --outfile models.py mssql+pyodbc......
#+end_src

The resulting file can be easily imported and the this task promptly abstracted.

** DONE Reading SQL Dumps with SQL Server management Studio   :sql:Databases:
CLOSED: [2020-04-12 dom 10:58]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-04-05-reading-sql-dumps-with-sql-management-studio
:END:

This post is about how to inspect the content of SQL database (/.mdf/ and /.ldf/ files). The answer is to attach these files to an existing SQL server instance, instead of opening them directly with a tool such as SQL Management Studio. This was not clear time until I stumbled upon [[https://www.youtube.com/watch?v=rhIr9Qf-oHw][this video.]]

*** Steps

1. Keep a SQL Server instance running in the background;
2. Fire up SQL Management studio and connect to this instance by providing /.\SQLEXPRESS/ in the "Server name" field;
3. Right click "Database" and attach the mdf file. The ldf is automatically included;

*** Attention points

To check #1 spin up /SQL Configuration Manager/ and look for **SQL Server
(SQLEXPRESS)** instance. It should be already running by default, as shown in
Figure 1.

#+caption: SQL Express running instance
#+name: img:sqlexpress
[[file:/img/sqlexpress.png]]

As for #2 make sure to login using the system credentials as shown in Figure 2.

#+caption: SQL Server login
#+name: img:sqlserver
[[file:/img/sqlserver.png]]

As for #3, if you are not admin the mdf file needs to be stored somewhere in the
 Public user profile so the SQLExpress instance is able to locate it.

** DONE Building Emacs from source                             :emacs:source:
CLOSED: [2020-04-11 sáb 10:59]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-04-10-building-emacs-from-source-instructions
:END:

Steps needed to install emacs locally, mantaining pre-existing installation intact. In the end you will two different versions of emacs running in your system.

- [[https://www.emacswiki.org/emacs/EmacsSnapshotAndDebian][Install dependencies:]]
#+begin_src bash
  sudo apt-get install autoconf automake libtool texinfo build-essential xorg-dev libgtk-3-dev libjpeg-dev libncurses5-dev libdbus-1-dev libgif-dev libtiff-dev libm17n-dev libpng-dev librsvg2-dev libotf-dev libgnutls28-dev libxml2-dev libxpm-dev
#+end_src
- [[https://www.gnu.org/software/emacs/download.html][Download emacs from a nearby mirror]]
- Extract downloaded file. In this case it is /emacs-26.3.tar.xz/, and go to the extracted folder:
#+begin_src bash
  cd ~/Downloads
  tar -xf emacs-26.3.tar.xz
  cd emacs-26.3.tar.xz
#+end_src
- Run /configure/ setting the prefix to the local folder in home directory:
#+begin_src bash
  ./configure --with-mailutils --prefix="${HOME}/local"
#+end_src
- Build the components and istall it:
#+begin_src bash
  make
  make install
#+end_src

To run it, issue:
#+begin_src bash
  /home/local/bin/emacs
#+end_src

The local installation, if any, should still work normally.

** DONE Upgrading an outdated Hugo template                            :hugo:
CLOSED: [2020-05-10 dom 18:04]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-05-10-upgrading-an-outdated-hugo-template
:END:

Recently, I wanted to restart blogging in a new machine. After installing the
latest hugo version at the time and pulling my git repo I soon realized:

1. Hugo version in which the blog was built was v0.31, whereas the current one
   was v0.68.1.
2. The cocoa development was abandonned and no updates were made to cope with
   hugo enhancements

Since I do like this theme and already had a fork of it I decided to try and
upgrade it by myself, even though I have no experience with the go programming
language or web development.

This post is to tell you how I went about it.

*** Finding where the errors come from

First of all, running the dated theme with new hugo raised the following errors:

#+caption: Running hugo serve on outdated template
#+name: oldtemplate
[[file:/home/gtpedrosa/Projects/blog/hugo-blog/static/img/hugo-warnings.png]]
*Hugo serve warnings on outdated template*

The first thing you may notice is that there is no mention to what part of the
template is raising the warnings being shown. To overcome this, I grepped the
offending structures and mapped which template files were triggering the
warnings:

#+caption: Finding out offending lines from hugo template
#+name: greptemplate
[[file:/home/gtpedrosa/Projects/blog/hugo-blog/static/img/hugo-warning-lines.png]]
*Hugo warning files and lines*

Just in case you are wondering here is a summary of the grep flags used:

- r or -R is recursive,
- n is line number, and
- w stands for match the whole word.

The next step involved fiddling with the source code and learning about each one of the errors.

*** Page.RSSLink is deprecated

I have found some github issues related to the matter, such as [[https://github.com/gohugoio/hugo/issues/4427][this one.]]
However, what solved my issue was to read the docs [[https://gohugo.io/templates/rss/][here]] and look how an up to dated
template should handle RSS. Replacing the bit using the /.RSSLink/ construct
with the snippet below solved the issue.

#+begin_src html
  {{ with .OutputFormats.Get "rss" -}}
      {{ printf `<link rel="%s" type="%s" href="%s" title="%s" />` .Rel .MediaType.Type .Permalink $.Site.Title | safeHTML }}
  {{ end -}}
#+end_src

*** Page.URL is deprecated

This error was easier to tackle. The warning gave a reasonable solution that
worked in my case. By replacing /Page.URL/ with /.Permalink/ everything worked
as expected. Note that there was some trial and error here since there are other
two other possible options according to the warning. I iterated until success.

*** Page.UniqueID is deprecated

Again the warning suggestion was spot on and replacing /.UniqueID/ with
/.File.UniqueID/ supressed the warning.

*** Most recent blog posts not showing on index.html

This was the trickiest to fix. I ended up reading a little bit about [[https://gohugo.io/templates/template-debugging/][how to
debug hugo templates.]] It so happens that after a couple of iterations printing
some of the variables in the /index.html/ file with the synthax

#+begin_src html
  {{ printf "%#v" .Permalink }}
#+end_src

I found out /.Data.Pages/ as not yielding the right ammount of posts.
Using /.Site.RegularPages/ did. I cannot find the exact resource which brought to
my attention the difference between the old and new hugo synthax, however, It is
agreeable that /.Data/ is too generic of a name for this use case.

*** Final result

As final result the theme is working without any warnings on hugo v0.68.1.

The changes applied to the theme are summarised by the git diff below. For more
reasonable view just check [[https://github.com/gtpedrosa/cocoa-hugo-theme][my fork of the cocoa-hugo theme.]]

#+begin_src shell :results output
  cd /home/gtpedrosa/Projects/blog/hugo-blog/themes/cocoa/
  git diff c6e7 434d
#+end_src

#+begin_src shell
  diff --git a/layouts/_default/list.html b/layouts/_default/list.html
  index b2348a1..4490e6c 100644
  --- a/layouts/_default/list.html
  +++ b/layouts/_default/list.html
  @@ -10,7 +10,7 @@
               <nav class="section-items">
                   <ul>
                   {{ range .ByWeight }}
  -                    <li><a {{ printf "href=%q" .URL | safeHTMLAttr }}>{{ default .Title .Params.heading }}</a></li>
  +                    <li><a {{ printf "href=%q" .Params.url | safeHTMLAttr }}>{{ default .Title .Params.heading }}</a></li>
                   {{ end }}
                   </ul>
               </nav>
  diff --git a/layouts/index.html b/layouts/index.html
  index be3a722..7d6252d 100644
  --- a/layouts/index.html
  +++ b/layouts/index.html
  @@ -7,12 +7,12 @@
                       {{ .Content }}
                   </div>
               {{ end }}
  -            {{ $totalpostscount := len (where .Data.Pages "Section" "blog") }}
  +            {{ $totalpostscount := len (where .Site.RegularPages "Section" "==" "blog") }}
               {{ $latestpostscount := .Site.Params.latestpostscount | default $totalpostscount }}
               {{ if gt $latestpostscount 0 }}
                   <div class="page-heading">{{ i18n "latestPosts" }}</div>
                   <ul>
  -                    {{ range (first $latestpostscount (where .Data.Pages.ByPublishDate.Reverse "Section" "blog")) }}
  +                    {{ range (first $latestpostscount (where .Site.Pages.ByPublishDate.Reverse "Section" "blog")) }}
                           {{ partial "li.html" . }}
                       {{ end }}
                       {{ if gt $totalpostscount $latestpostscount }}
  diff --git a/layouts/partials/head_includes.html b/layouts/partials/head_includes.html
  index 3c21e39..24723e2 100644
  --- a/layouts/partials/head_includes.html
  +++ b/layouts/partials/head_includes.html
  @@ -51,9 +51,9 @@
   >

   <!-- RSS -->
  -{{ if .RSSLink }}
  -  <link href="{{ .RSSLink }}" rel="alternate" type="application/rss+xml" title="{{ .Site.Title }}" />
  -{{ end }}
  +{{ with .OutputFormats.Get "rss" -}}
  +    {{ printf `<link rel="%s" type="%s" href="%s" title="%s" />` .Rel .MediaType.Type .Permalink $.Site.Title | safeHTML }}
  +{{ end -}}

   <!-- gitalk -->
   {{ if .Site.Params.gitalk }}
  diff --git a/layouts/partials/staticman/form-comments.html b/layouts/partials/staticman/form-comments.html
  index 91067df..544301c 100644
  --- a/layouts/partials/staticman/form-comments.html
  +++ b/layouts/partials/staticman/form-comments.html
  @@ -1,6 +1,6 @@
   <form method="POST" action="https://api.staticman.net/v2/entry/{{ .Site.Params.staticman.username }}/{{ .Site.Params.staticman.repository }}/{{ .Site.Params.staticman.branch }}/">
       <input type="hidden" name="options[redirect]" value="{{ .Permalink }}#comment-submitted">
  -    <input type="hidden" name="options[entryId]" value="{{ .UniqueID }}">
  +    <input type="hidden" name="options[entryId]" value="{{ .File.UniqueID }}">
       <input name="fields[name]" type="text" placeholder="Your name">
       <input name="fields[email]" type="email" placeholder="Your email address">
       <textarea name="fields[body]" placeholder="Your message. Feel free to use Markdown." rows="10"></textarea>
  diff --git a/layouts/partials/staticman/show-comments.html b/layouts/partials/staticman/show-comments.html
  index bba3b5c..d7ff90e 100644
  --- a/layouts/partials/staticman/show-comments.html
  +++ b/layouts/partials/staticman/show-comments.html
  @@ -1,6 +1,6 @@
    {{ $comments := readDir "data/comments" }}
    {{ $.Scratch.Add "hasComments" 0 }}
  - {{ $entryId := .UniqueID }}
  + {{ $entryId := .File.UniqueID }}

    {{ range $comments }}
      {{ if eq .Name $entryId }}
#+end_src

** DONE Moving files on windows with python: shutil alternative
CLOSED: [2020-05-17 dom 11:20]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-05-17-moving-files-on-windows-with-python-shutil-alternative
:END:

Transfering data from my HD across a network, I have hit a bottleneck: the file
transfer speed was dramatically slow. In fact, the transfer took more time to finish than it took to generate the files. It was an unnaceptable situation due HD constraints.

It turns out that the file being copied to another partition is virtually
chunked, and each of them copied sequentialy. The more chunks the slower the
transfer is, while larger chunks increase memory usage [[https://superuser.com/questions/558292/how-does-copy-and-paste-for-large-files-work][[1]​]]. Iniatially I thought
I had to change the shutil module itself to make it work, so I avoided this
path [[https://stackoverflow.com/questions/21799210/python-copy-larger-file-too-slow][[2]​]].

The solution I came up, however, was switching to Windows Robust File Copy utility, or ~robocopy~. It has some perks, including showing the status of the
transfer on the command line, which was handy at this particular time. The
solution looks like the following:

#+begin_src python
  import time
  import os

  source = '\\source\\folder'
  target = '\\target\\folder'

  while True:
      for filename in os.listdir(source):
          os.system('Robocopy "%s" "%s" "%s" /MOV' %
                    (source, target, filename))
      time.sleep(300)
#+end_src

The python usage was only necessary to periodically move incoming files, keeping HD usage controlled.

#+begin_quote
**Attention:** Do not use ~/MOVE~ unless you want the folder structure to be
  moved as well. ~/MOV~ will keep the folder structure intact, moving only the
  files targeted.
#+end_quote

Later I have found out you can manage to adjust buffer size directly
without modifying the source code directly [[https://blogs.blumetech.com/blumetechs-tech-blog/2011/05/faster-python-file-copy.html][[3]​]]. I will keep that in mind for the
next time.

** DONE Setting up a Data Science local environment without administrative rights on Windows :data:science:windows:R:Python:Git:editors:IDE:
CLOSED: [2020-06-07 dom 18:24]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-06-07-data-science-local-environment-without-administrative-rights-on-windows
:END:

Installing any program on ~C:\Program Files~, Windows default installation folder, requires administrative rights. It is possible, however, to install programs on the ~HOME~ folder without any special permissions. The default ~HOME~ path is set to ~C:\Users\yourusername\~ and can be accessed by typing ~%HOME%~ on the address bar. I have found this approach makes it possible to have some autonomy from the IT department without compromising security.

Using this approach, the softwares will run just fine without any customization. One caveat, however, is that some set up is needed in order to make them interact with one another properly. 

The following list is a personal preference related to the tools used to perform the data science tasks I found easiest to setup without administrative (admin) rights and how to make them work without any problems.

*** Softwares

The main softwares I use are listed below:

- **Languages:** I use R mainly for data exploration/prototyping and Python for solution development. Bear in mind, however, that you can get some mileage with R exclusively depending on the need;
- **Editors:** I have installed notepad++, vim, emacs and Sublime Text 3. I end up using notepad++ for quick inspections of files, emacs for orgmode exclusively and Sublime for all my Python needs;
- **IDEs:** Why an IDE if you have a bunch of editors you might ask. Well, I find the experience of using RStudio for data explorations using R very well streamlined. Packaging works, I can do literate programming and weave the code into reports that work as an engineering memorandum and a reference for my future self or present analysis results;
- **VCS:** Git, indispensable in the analysis workflow and necessary to expand the toolbox set by pulling packages;

*** Configuration
**** Portable Programs

[[https://www.sublimetext.com/3][Sublime Editor]], [[https://git-scm.com/download/win][Git]], [[https://cran.r-project.org/bin/windows/base/][R]], [[https://rstudio.com/products/rstudio/download/][RStudio]] and [[https://www.python.org/downloads/windows/][Python]] all have portable versions, which do not require any installation. If not, they allow you to proceed with the installation process selecting a folder of your choice, such as ~HOME~.

The downfall of this installation process is that none of the above programs will be on the system path, nor they will be available through the command line. Here is where the ~aliases.cmd~ comes in.

**** aliases.cmd

To make a program available system wide, it's path should be appended to the ~PATH~ variable. Unfortunately, doing this permanently is also a privileged operation. It is possible however, to append the new paths every time you start a terminal for instance. To accomplish this, you can take the following approach, borrowed from [[https://stackoverflow.com/questions/20530996/aliases-in-windows-command-prompt][this SO answer]]: 

1. Create a .bat or .cmd file with your DOSKEY commands.
2. Run regedit and go to HKEY_CURRENT_USER\Software\Microsoft\Command Processor
3. Add String Value entry with the name AutoRun and the full path of your .bat/.cmd file.
 
In my case, I have created a ~alias.cmd~ file in my ~HOME~ folder. The file is reproduced below:

   #+begin_src cmd
  @echo off

  :: Temporary system path at cmd startup

  set PATH=%PATH%;

  :: Commands

  DOSKEY ls=dir /B
  DOSKEY npp="C:\Users\yourusername\npp\notepad++.exe"
  DOSKEY alias=notepad %USERPROFILE%\alias.cmd
  DOSKEY vim="C:\Users\yourusername\Vim\vim81\gvim.exe"
  DOSKEY p1=cd "C:\Users\yourusername\p1" ^& "venv\Scripts\activate"
  DOSKEY va="venv\Scripts\activate"
  DOSKEY vd="venv\Scripts\deactivate"

  set curldir="C:\Users\yourusername\curl\bin\curl.exe"
  set gitdir="C:\Users\yourusername\PortableGit\cmd"
  set jupyterdir="C:\Users\yourusername\appdata\local\programs\python\python37\Scripts"
  set subldir="C:\Users\yourusername\Sublime"
  set rcdir="C:\Program Files (x86)\Windows Kits\10\bin\10.0.17763.0\x64"
  set bashesdir="C:\Users\yourusername\bashes"
  set cmakedir="C:\Users\yourusername\cmake\bin"

  set pythonpath="C:\Users\yourusername\AppData\Local\Programs\Python\Python37"
  set pythonscripts="C:\Users\yourusername\AppData\Local\Programs\Python\Python37\Scripts"
  set rdir="C:\Users\yourusername\R\R-3.5.1\bin"
  set PATH=%curldir%;%gitdir%;%cmakedir%;%pythonpath%;%pythonscripts%;%subldir%;%rcdir%;%bashesdir%;%rdir%;%PATH%
  #+end_src

Some interesting points about this configuration:

- Typing ~alias~ in the terminal brings me to the file above so I can tweak it easily;
- Part of my workflow with python relies on using virtual environments. I have a standardized name for my virtual environments which is ~venv~. Therefore, I alieased the actiavtion/deactivation from the project root to ~va~ and ~vd~. This works across all my projects;
- Usual projects received their own aliases, such as the ~p1~ example;
- Python, Git, curl and R could are appended to the system path;

Now, either running a new terminal or executing the software, you should feel no difference in usage regardless of how/where the tools were installed.

*** Failures

This blog post would not be complete if I did not talk about the things i could **not** get to work. 

**** WSL
I have previously used Windows Subsystem for Linux (WSL) with success for personal projects. In fact, I believe that the [[https://nickjanetakis.com/blog/a-linux-dev-environment-on-windows-with-wsl-docker-tmux-and-vscode][setup proposed]] by Nick Janetakis is great. However, even asking for a privileged user to install and enable the WSL, I could not get it running with my unprivileged user. 
**** Docker
Similarly to what happened to the WSL, I have also tried to make docker available without any success. It seemed that all the changes made by a privileged user did not persist when I tried to use the installed software with my unprivileged credentials. 
**** Apache airflow
Apache airflow could not be installed without a admin rights. 

In fact it should, but I did not have Microsoft Visual C++ Build Tools in my system, as per the error below.

#+begin_quote
error: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft Visual C++ Build Tools": https://visualstudio.microsoft.com/downloads/
#+end_quote

Since it was not a pre requesite for my projects, I have not investigated any further.

**** MikTex
I still have not figured out how to properly set MikTex to R's path. Miktex has a portable edition that can be downloaded from [[https://miktex.org/download][MikTex download page]]. As of now, weaving documents to pdf are still unavailale in my system and will require further integration.

*** Closing Notes

This is a simple, yet useful standard of tools and ways of operating with them. It should be noted:

- I have not been working with tools for front-end, but did have npm installed recently to make slide decks with [[https://revealjs.com/][reveal.js]] using the same approach;
- My analysis are not really considered data-heavy. In the cases where they were more intensive, relying on libraries such as [[https://diskframe.com/][diskframe]] or caches worked fine;

Let me know if you managed to get WSL, docker or Miktex working, or have a set-up willing to share.
** DONE Enabling Frescobaldi MIDI output         :MIDI:lilypond:Frescobaldi:
CLOSED: [2020-06-21 dom 23:29]
:PROPERTIES:
:EXPORT_FILE_NAME: 2020-06-21-enabling-frescobaldi-midi-output
:END:

Trying to export a composition engraved with lilypond to MIDI, I have found out that it was not properly set up. The resulting MIDI was mute, even though I could see its contents using Audacity.

The solution lies in [[https://askubuntu.com/questions/463575/frescobaldi-midi-player-seems-to-be-working-fine-but-doesnt-produce-any-sound][this post]]. After installing a MIDI to Wave converter and player called timidity and starting its daemon:

#+BEGIN_SRC shell
sudo apt-get install timidity
sudo service timidity start
#+END_SRC

A /Timidity port 0/ should be available after refreshing Midi Settings as player output option in the /Edit > Preferences > Midi Settings/ in Frescobaldi.

** DONE Patterns in Circle of Fiths                     :music:theory:piano:
CLOSED: [2021-11-30 ter 21:09]
:PROPERTIES:
:EXPORT_FILE_NAME: 2021-11-30-patterns-in-circle-of-fiths
:END:

Last Sunday I decided to start (re)learning some music theory by myself. The objective was to learn the scales and the entrypoint for that was the circle of fiths. I don't know why, but I felt the need to hold the circle of fiths chart I was consulting in my own hands. So instead of printing it I made my own, and while I was at it I thought it would be cool if the inner circle could spin, so I made it likewise. Here's the outcome:

#+caption: Handmade circle of fiths
#+name: cof
[[file:/home/gtpedrosa/Projects/blog/hugo-blog/static/img/circle_of_fiths.jpg]]
*Handmade circle of fiths chart*

The phisicality of it made me eager to play with this tool. Soon, I started gaining insights that might be obvious for a seasoned musician or someone who is proficient in theory. Nonetheless, here are some findings that I found amusing.

*** 1.Notes Interwined

Notice how by starting at **F**, it is possible to spot the following sequence: F_G_A_B. The underscores just mean that a note is skipped. The interesting part is that you can fill the gaps with another sequence: _C_D_E_, yielding the full sequence FCGDAEB. So 7/12 notes of the circle are easily spotted as the white notes on the piano. But what about the other 5/12?

*** 2.Accidental Simmetry

Well, if you pay close attention between **F** and **B** counterclockwise all notes have accidentals. Consider the sequence GDAEB, which are the notes of the circle clockwise after C. Now repeat them with a flat and you have found the remaining 5/12 notes from the circle!

*** 3.Identifying key from Key Signature

**** 3.1.Sharps

Add a semitone to the last sharp, or roughly add a note to it. For instance, what is the major key for a key signature with a single sharp (F#)? F+1=G.

**** 3.2.Flats

Drop the last flat. It does not work with the major key signature with only one flat, which is F. For instance **Bb** major has two flats: B and E. Drop the Eb (last flat in the signature) and you will have Bb itself.

*** 4.Sharps and Flats Relationship

Being a circle one could wonder whereas sharps and flats are interconnected. If we take a closer look at the sharps and flats sequences:

- **Sharps:** FCGDAEB
- **Flats:** BEADGCF

If you read the sharps backwards you get the flats! Also, the sharps are just the sequence of the circle of the fiths ranging from F to B clockwise. Just refer to the image and it is easy to verify. If you go counterclockwise from B to F you get the flats. 

*** 5.From Major to Minor and Vice-Versa

The major and minor scales can be obtained through the following interval composition:

- **Major:** TTSTT(TS)
- **Minor:** (TS)TTSTT

Here T stands for tone and S for semi-tone. The funny thing is, if you remove the last two intervals of a major scale (TS) and transfer it to the beginning of the remaining intervals (TTSTT) you get the minor scale formulation. This realization is even cooler in the circle itself:

#+caption: Spun inner circle of the circle of fiths
#+name: spuncof
[[file:/home/gtpedrosa/Projects/blog/hugo-blog/static/img/circle_of_fiths_spun.jpg]]
*Spun inner circle*

Here if you spin the inner circle by 1,5 tones (3 notes) you arrive at the same configuration as the major scale. So to obtain the relative minor to a major scale remove 1,5 tones from it. To derive the major from the minor add 1,5 tones. Example: What is the relative minor for D major? B minor. Try to see it on a piano, it is way easier. Another example the other way: What is the relative major scale to C# minor? E major. 
** DONE Migrating CI tool from travis to circleci for hugo blog on ghpages :dev:ops:hugo:ghpages:circleci:
CLOSED: [2021-12-05 dom 09:11]
:PROPERTIES:
:EXPORT_FILE_NAME: 2021-12-05-migrating-ci-tool-from-travis-to-circleci-for-hugo-blog-on-ghpages
:END:

When Travis CI was acquired by Idera, it didn't take long to cease its support for open source projects. This affected this blog since I overengineered it to learn some dev-ops skills. As result, I was forced to migrate the continuous integration (CI) tool from Travis to CircleCI. While I was at it, I took advantage and included a docker image for the build instead of declaring the dependencies installation as part of the CI build.

I am indebted to [[https://github.com/z0li/hugo-builder][z0li]] who provided a guide I was able to follow through [[https://z0li.github.io/deliver-static-sites-with-hugo-circleci-github/][here.]] The Docker image, config.yml and deploy scripts were modified from z0li's post.


*** 1.Docker Image 

This is the base Docker image upon which the blog is built. It still contains htmlproofer even though I am not using it at the moment. It is something either I'll adopt or drop in the future. It also contains, of course, hugo version 0.68.1 which is the last one I troubleshooted the cocoa theme for as detailed in my [[https://gtpedrosa.github.io/blog/upgrading-an-outdated-hugo-template/][previous post.]]

#+begin_src Dockerfile
FROM ruby:2.6-alpine3.9

ENV HUGO_VERSION=0.68.1

RUN apk add --no-cache make gcc libc-dev bash libcurl ruby-nokogiri \
      openssh-client rsync git && \
    gem install --no-document html-proofer

RUN mkdir /tmp/hugo && \
    cd /tmp/hugo && \
    wget https://github.com/spf13/hugo/releases/download/v${HUGO_VERSION}/hugo_${HUGO_VERSION}_Linux-64bit.tar.gz && \
    tar xzvf hugo_${HUGO_VERSION}_Linux-64bit.tar.gz && \
    mv /tmp/hugo/hugo /usr/local/bin/ && \
    rm -rf /tmp/hugo

CMD [ "hugo", "version" ]
#+end_src

*** 2. config.yml

This is where most of the action happens, the *config.yml* file required by CircleCI.

#+begin_src config.yml
version: 2.1
jobs:
  build:
    docker:
      - image: gtpedrosa/hugo-build:latest
    working_directory: /src
    steps:
      - add_ssh_keys:
          fingerprints:
            - "x4TBLAoCPktmU+ECc3aoxBPuymLldWBEaFpnR8yFDdE"
      - checkout
      - run: git submodule update --init
      - run: hugo -v -s /src -d /src/public
      - deploy:
          name: push to master branch
          command: sh /src/scripts/deploy_ci.sh
#+end_src

Notice:
- docker: it pulls the image previously uploaded to dockerhub and [[https://github.com/gtpedrosa/hugo-build/blob/master/Dockerfile][linked to my github repo.]]
- working_directory: specify the location to work from
- add_ssh_keys: include necessary credentials so circleci communicates to github
- checkout: special command to checkout the gtpedriosa.github.io repo
- first run: update submodules, in this case, the hugo theme used and [[https://github.com/gtpedrosa/cocoa-hugo-theme][fork that I mantain]], but designed by Nishanth Shanmugham
- second run: after pulling the code it runs hugo and saves the html in the usual *public* folder
- deploy: calls deploy script where the rest of action happens, which I'll document in the following section.

*** 3. deploy_ci.sh

Admitedly, I made inumerous mistakes in this transition. And learned a lot by trial and error, so here's the full file:

#+begin_src deploy_ci.sh
#!/bin/bash
set -e

echo "* checking out the master branch:"
git clone --single-branch --branch master git@github.com:gtpedrosa/gtpedrosa.github.io.git master

echo "* synchronizing the files:"
rsync -arv /src/public/ master --delete --exclude ".git"
cp README.md master/

echo "* pushing to master:"
cd master
echo "cd OK"
git config user.name "CircleCI"
git config user.email "guilherme.pedrosa@gmail.com"
echo "git config OK"
git add -A
echo "git add OK"
git commit -m "Automated deployment job ${CIRCLE_BRANCH} #${CIRCLE_BUILD_NUM} [skip ci]" --allow-empty
echo "git commit OK"
git push origin master

echo "* done"
#+end_src

Now, let's break it down in three parts and make some comments.

#+begin_src p1
set -e
#+end_src

The first excerpt causes the execution of the script to fail if an error is raised, instead of the default behavior of ignoring it. Therefore, any issues in the deployment will cause the build to fail.

#+begin_src p2
echo "* checking out the master branch:"
git clone --single-branch --branch master git@github.com:gtpedrosa/gtpedrosa.github.io.git master

echo "* synchronizing the files:"
rsync -arv /src/public/ master --delete --exclude ".git"
cp README.md master/
#+end_src

The html files in the master branch are cloned in this second part. The generated html pages from the hugo run (refer to the second run statement in the /config.yml/ file) are synced to the folder where the html files were cloned. Notice where they are coming from in the rsync argument. This is basically where the new posts are added to the previous ones.

#+begin_src p3
echo "* pushing to master:"
cd master
echo "cd OK"
git config user.name "CircleCI"
git config user.email "guilherme.pedrosa@gmail.com"
echo "git config OK"
git add -A
echo "git add OK"
git commit -m "Automated deployment job ${CIRCLE_BRANCH} #${CIRCLE_BUILD_NUM} [skip ci]" --allow-empty
echo "git commit OK"
git push origin master

echo "* done"
#+end_src

Lastly, the changes are commited and pushed to master. This will update the ghpages site automatically. One detail to note is the /[skip ci]/ tag. From the documentation:

> By default, CircleCI automatically triggers a pipeline whenever you push changes to your project. You can override this behavior by adding a [ci skip] or [skip ci] tag within the first 250 characters of the body or title of the commit. 

If you ommit it, another job will be triggered and fail. This is messing up with the badge in the readme of the repository at this exact moment.

Also, any issue in the build can be debugged in the newly spun environment by [[https://circleci.com/docs/2.0/ssh-access-jobs/][sshing in an interactive shell.]] I really appreciated this to understand where I messed up with my paths.

Well, I hope this breakdown distilled some of the learnings in the process.

** DONE Installing Linux on HP 246 G6                  :linux:debian:driver:
CLOSED: [2022-01-29 sáb 15:43]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-01-29-installing-linux-on-hp-246-g6
:END:

Trying to speed-up my notebook HP 246 G6, I decided to upgrade my HD to a SSD. While at it, I have also decided to switch from Windows to Ubuntu. However, things did not go as smooth as I thought they would.

Firstly, I could not flash a new Ubuntu image successfully on my 16GB CruzerBlade thumbdrive. I used the [[https://www.balena.io/etcher/][Balena Etcher electron app]]. In case you have never used it before, just issue the following in the command line to get it running:

#+begin_src bash
sudo ./balenaEtcher-1.7.3-x64.AppImage
#+end_src  
 
 I do not remeber why, but it is worth noticing that running without privileges I ran into trouble. Same thing for *Gparted* used to format the CruzerBlade. I had to run it with **sudo** in order to work and format the thumbdrive, which would not work otherwise.

Image flashed, I moved on to installation. To select the booting device on a HP machine, press **F9**. Ubuntu GUI for installation appeared after HD check, but it would freeze before the installation process effectively started. Mind you this was Ubuntu 20.04 and found [[https://appuals.com/how-to-fix-ubuntu-20-04-installer-stuck-at-updates/][I was not alone with this issue]]. Without any success resolving it, I decided to give Debian a try.

I chose to download the DVD image, and re-flashed the CruzerBlade. Installation ran smoothly until the "Setup connection" part. It would not recognize my Wifi driver. After installation, I had add my user to sudo:

#+begin_src bash
su -
usermod -aG sudo username
exit
#+end_src

And restart the PC to take effect. To see if it worked, try to find **sudo** in the groups listed by 

#+begin_src bash
groups
#+end_src

Before issuing any commands, I also had to remove the "CD Rom" drives as source for other software in Synaptic. Otherwise, no internet mirrors were reached for updates. Now I could finally update the system and download new software.

First thing to do is to findout the driver, which was easy after some search for the right command:

#+begin_src bash
lspci
#+end_src

It gave me that my Wireless Network Adapter was the Realtek RTL8723BE. Again, I was not the only one in trouble and found an excellent guide [[https://trendoceans.com/how-to-fix-rtl872be-no-wifi-network-connection-in-debian/][here.]] However, My fix was much shorter as It was already bundled in a [[https://debian.pkgs.org/10/debian-nonfree-amd64/firmware-realtek_20190114-2_all.deb.html][debian mirror in version 10]] and I had just installed Debian 11 LTS. So after installing it (with mirrors working now):

#+begin_src bash
sudo apt install firmware-realtek
#+end_src

I had it working. I am writing this as a reminder to my future self of this little saga to get the notebook up and running.

** DONE Fixing ever growing kern.log, messages and syslog files on Debian :debian:linux:
CLOSED: [2022-03-06 dom 22:49]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-03-06-fixing-ever-growing-kern-dot-log-messages-and-syslog-files-on-debian
:END:

After a Fresh install of Debian, I was suddenly surprised by the current lack of disk space to save a file. This post is a memo for my future self of how I managed to solve this issue.

In summary, the lack of disk space was caused by ever-growing logging messages stored on three files located in */var/log*: messages, kern.log, and syslog. What messages? Specifically these:

#+begin_src 
guilherme@gtpedrosa: sudo tail syslog
Mar  5 12:11:50 gtpedrosa kernel: [  256.873658] pcieport 0000:00:1d.0: AER: Corrected error received: 0000:00:1d.0
Mar  5 12:11:50 gtpedrosa kernel: [  256.873661] pcieport 0000:00:1d.0: PCIe Bus Error: severity=Corrected, type=Physical Layer, (Receiver ID)
Mar  5 12:11:50 gtpedrosa kernel: [  256.873662] pcieport 0000:00:1d.0:   device [8086:9d18] error status/mask=00000001/00002000
Mar  5 12:11:50 gtpedrosa kernel: [  256.873662] pcieport 0000:00:1d.0:    [ 0] RxErr                 
Mar  5 12:11:50 gtpedrosa kernel: [  256.873669] pcieport 0000:00:1d.0: AER: Corrected error received: 0000:00:1d.0
Mar  5 12:11:50 gtpedrosa kernel: [  256.873679] pcieport 0000:00:1d.0: AER: can't find device of ID00e8
Mar  5 12:11:50 gtpedrosa kernel: [  256.873680] pcieport 0000:00:1d.0: AER: Corrected error received: 0000:00:1d.0
Mar  5 12:11:50 gtpedrosa kernel: [  256.873684] pcieport 0000:00:1d.0: PCIe Bus Error: severity=Corrected, type=Physical Layer, (Receiver ID)
Mar  5 12:11:50 gtpedrosa kernel: [  256.873688] pcieport 0000:00:1d.0:   device [8086:9d18] error status/mask=00000001/00002000
Mar  5 12:11:50 gtpedrosa kernel: [  256.873689] pcieport 0000:00:1d.0:    [ 0] RxErr   
#+end_src

Having no clue how to proceed, the following resources shed some light on how I should approach the issue:

- [[https://gist.github.com/Brainiarc7/3179144393747f35e5155fdbfd675554][Github gist]]
- [[https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1521173][Launchpad]]
- [[https://unix.stackexchange.com/questions/195360/my-var-log-is-mysteriously-filling-up-gbs-in-minutes-any-cure-before-i-re-ins][StackExchange]]

Following the gist resource, I found that the offending Bus error was owed to my PCI (/Peripheral Computer Interface/) bridge. The bus info is exactly the one reported in the log files:

#+begin_src 
guilherme@gtpedrosa:/var/log$ sudo lshw -numeric
        *-pci:2
             description: PCI bridge
             product: Sunrise Point-LP PCI Express Root Port #9 [8086:9D18]
             vendor: Intel Corporation [8086]
             physical id: 1d
             bus info: pci@0000:00:1d.0
             version: f1
             width: 32 bits
             clock: 33MHz
             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list
             configuration: driver=pcieport
             resources: irq:124 ioport:3000(size=4096) memory:b1000000-b10fffff
           *-network
                description: Wireless interface
                product: RTL8723BE PCIe Wireless Network Adapter [10EC:B723]
                vendor: Realtek Semiconductor Co., Ltd. [10EC]
                physical id: 0
                bus info: pci@0000:03:00.0
                logical name: wlo1
                version: 00
                serial: b0:52:16:ff:33:9f
                width: 64 bits
                clock: 33MHz
                capabilities: pm msi pciexpress bus_master cap_list ethernet physical wireless
                configuration: broadcast=yes driver=rtl8723be driverversion=5.10.0-11-amd64 firmware=N/A ip=192.168.15.15 latency=0 link=yes multicast=yes wireless=IEEE 802.11
                resources: irq:16 ioport:3000(size=256) memory:b1000000-b1003fff
               
#+end_src

After identifying the key information I tried to rewrite the base registry manually. The current output was:

#+begin_src 
guilherme@gtpedrosa:/var/log$ sudo setpci -v -d 8086:9d18 CAP_EXP+0x8.w
0000:00:1d.0 (cap 10 @40) @48 = 000f
#+end_src

And the expected output should be *000e* (according to the gist author). However, after trying to overwrite it:

#+begin_src 
guilherme@gtpedrosa:/var/log$ sudo setpci -v -d 8086:9d18 CAP_EXP+0x8.w=0x0e
pcilib: sysfs_write: write failed: Operation not permitted
0000:00:1d.0 (cap 10 @40) @48 000e
#+end_src

I got permission denied. **The only way to make sysfs_write work was to disable secure boot**. Only then, the above command ran without issue. It took me quite a while to figure this piece out. However, this only halted the logs in the current session, meaning that after a restart it would grow again until I re-issued the command. This led me to alter the grub file, as per the launchpad suggestion. The final answer involved editing */etc/default/grub* and replacing the existing *GURB_CMD_LINE_LINUX_DEFAULT* with the following:

#+begin_src 
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash pci=noaer" 
#+end_src

Emphasis on the *pci=noaer* option. What this does, according to the [[https://www.kernel.org/doc/html/v4.14/admin-guide/kernel-parameters.html][Linux Kernel manual]], is to disable the PCI Express advanced logging. Note that *grub-update* is necessary. This was only possible after disabling secure boot as well. The /how-to/ is in one of the answers of the StackExchange link.

Sadly, this is only a workaround. Probably, as per [[https://gtpedrosa.github.io/blog/installing-linux-on-hp-246-g6/][my previous post]], and [[https://askubuntu.com/questions/1104219/what-does-pci-noaer-or-pci-nomsi-mean][this insight]], a hardware (my wireless driver) does not communicate with the kernel properly with MSI. Nonetheless, the HP notebook is ready for some serious work.

** DONE Setting up Debian 11 on my HP Pavillion dv7-6199us :linux:debian:driver:Ansible:chezmoi:
CLOSED: [2022-03-20 dom 17:46]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-03-20-setting-up-debian-11-on-my-hp-pavillion-dv7-6199us
:END:

The HP Pavillion dv7-6199us has been my notebook for over 10 years. I accidentally trashed my system and could not fix it when it crashed during an upgrade. This led me to format it over, which was long due. Taking advantage of my previous experience with my other HP, the 246, I tried to install Debian 11 instead of Ubuntu. Here are my other findings during the OS switch.

Debian does not come with non-libre software. This is the reason the wifi driver was not supported [[https://gtpedrosa.github.io/blog/installing-linux-on-hp-246-g6/][previously]]. Following my notes, I was able to quickly set this up. Interestingly, I have found that the drivers are located at */lib/firmware* which I did not know before trying a manual install. I ended up installing all the available driver options by issuing 

#+begin_src bash
# Wifi proprietary driver 
sudo apt update && sudo apt install firmware-iwlwifi
ls /lib/firmware
#+end_src

Meanwhile, I have noticed the fan noise pretty loud since the startup. I tracked this issue to be related to my graphics card. I do not recall where I have found this info, but updating the graphic drivers did the trick:

#+begin_src bash
# Avoid fan overwork
sudo apt-get install firmware-amd-graphics libgl1-mesa-dri libglx-mesa0 mesa-vulkan-drivers xserver-xorg-video-all
#+end_src

These were quirks related to this device only. However, due to my recent system installation, I thought about how could I automate this process. My research showed that Ansible is quite a powerful tool worth exploring in this area. For simplicity sake, my first approach was to keep a bash script. It tracked all the software I needed and the commands necessary to set them up. I hope to turn it into an Ansible playbook in the future. Meanwhile here's the bash script:

#+begin_src bash
# curl
sudo apt install curl

# pass
sudo apt install pass -y

# Install keychain to start ssh-agent if it isn't running
# https://stackoverflow.com/questions/18880024/start-ssh-agent-on-login
sudo apt install keychain -y

# gpg keys
gpg --import ~/.gnupg/pubring.gpg
gpg --import ~/.gnupg/secring.gpg
# https://stackoverflow.com/questions/33361068/gnupg-there-is-no-assurance-this-key-belongs-to-the-named-user
# gpg --edit-key <KEY-ID>
# trust
# 5
# quit

# Dropbox
cd ~ && wget -O - "https://www.dropbox.com/download?plat=lnx.x86_64" | tar xzf -
~/.dropbox-dist/dropboxd

# Spotify
curl -sS https://download.spotify.com/debian/pubkey_5E3C45D7B312C643.gpg | sudo apt-key add - 
echo "deb http://repository.spotify.com stable non-free" | sudo tee /etc/apt/sources.list.d/spotify.list
sudo apt-get update && sudo apt-get install spotify-client

# gnucash
# Install sqlite3 backend
sudo apt install libdbd-sqlite3 gnucash -y

# Install applet to fix battery status ("Estimating..." indefinetly")
#sudo add-apt-repository ppa:iaz/battery-status && sudo apt-get update 
#sudo apt-get install battery-status

# Editors
sudo apt install emacs neovim -y
# emacs uninstall org 9.3 and org related packages. Install spacemacs-theme. Restart.

# Fish shell
sudo apt install fish -y

# i3
sudo apt install i3 -y

# librewolf
echo "deb [arch=amd64] http://deb.librewolf.net $(lsb_release -sc) main" | sudo tee /etc/apt/sources.list.d/librewolf.list
sudo wget https://deb.librewolf.net/keyring.gpg -O /etc/apt/trusted.gpg.d/librewolf.gpg
sudo apt update
sudo apt install librewolf -y

# dotfiles
sudo sh -c "$(curl -fsLS chezmoi.io/get)" -- init --apply gtpedrosa
sudo mv ./bin/chezmoi /bin/
rm -rf ./bin
# OBS: emacs agenda only worked after reinstalling org package. Theme was not applied

# pyenv
sudo curl https://pyenv.run | bash

# Calibre
sudo -v && wget -nv -O- https://download.calibre-ebook.com/linux-installer.sh | sudo sh /dev/stdin

# Zotero
# download latest version manually

# Hugo
# download latest version form github
https://github.com/gohugoio/hugo/releases/download/v0.95.0/hugo_0.95.0_Linux-64bit.deb
sudo dpkg -i ./Downloads/hugo_0.95.0_Linux-64bit.deb

# htop
sudo apt install htop

# frescobaldi
sudo apt install frescobaldi

# FreeCAD
sudo apt install freecad
# Download ODA converter for dwg import and install
# https://www.opendesign.com/guestfiles/oda_file_converter

#+end_src

This episode was a great learning experience OS-wise. As a side effect, I got my dotfiles cruft reduced and made them more portable. By the way, shoutout to [[https://www.chezmoi.io/][chezmoi]], which made the setup of the dotfiles a breeze.

** DONE Initializing aliases on cmd startup :windows:cmd:registry:
CLOSED: [2022-05-22 Sun 21:09]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-05-22-initializing-aliases-on-cmd-startup
:END:

On my [[https://gtpedrosa.github.io/blog/setting-up-a-data-science-local-environment-without-administrative-rights-on-windows/][previous post]] about setting up a data-science development environment without admin rights on Windows, there is a step where the registry needs to be edited. Recently, I have tried to follow the same instructions I wrote back then without success. It happens that the Command Processor entry cannot simply be found anymore.

In order to make my aliases stored on HOME directory initialize upon cmd startup, I have found the following work around:

#+begin_src bash
reg add "HKCU\Software\Microsoft\Command Processor" /v AutoRun /t REG_EXPAND_SZ /d "%"USERPROFILE"%\alias.cmd" /f
#+end_src

It correctly autoruns the alias on cmd startup, solving the issue. More info on Microsoft’s docs.

** DONE My first OpenSCAD Project :OpenSCAD:CAD:carpentry:
CLOSED: [2022-06-12 Sun 09:55]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-06-12-my-first-openscad-project
:END:

I am trying my hand at carpentry and wanted to do some experimentations with a bookshelf of my own design. This led me to a search for a parametric CAD tool, where I could change determined design variables and have the model updated. OpenSCAD fitted the bill.

In under one hour I went from zero OpenSCAD knowledge to have a working model. The code itself is not optmized, but I was happy with the result. As resources go, I would recommend:

- [[https://www.youtube.com/watch?v=ecd_eWPnynk&t=46s][Model a Code Brick by mathcodeprint]]
- [[https://openscad.org/cheatsheet/][OpenSCAD Cheatsheet]]

By following them I was able to produce a 3D model from a notebook sketch. Here's the result with the accompanying /.scad/  code:


#+caption: Book Shelf model on OpenSCAD
#+name: cof
[[file:~/Projects/blog/hugo-blog/static/img/estante2.png]]
*Bookshelf model on OpenSCAD*


#+begin_src
//Variáveis paramétricas
pe=70;
h_gaveta=120;
esp_comp=15;
nicho=250;

//Pés
difference(){
    cube([750,300,1450]);
    translate([45,0,0])
    cube([660,300,1450]);
    translate([0,45,0])
    cube([750,210,1450]);
}

// Estantes
for (i=[0:3]){
translate([30,0,pe+250*i])
cube([690,300,15]);
}
translate([30,0,pe+250*3+h_gaveta])
cube([690,300,15]);
translate([30,0,pe+250*4+h_gaveta])
cube([690,300,15]);

//Gaveta
translate([45,0,pe+250*3])
cube([660,15,h_gaveta]);

// Puxador
translate([45+330,0,pe+250*3+h_gaveta/2]){
sphere(r=20);
}
// Apoios laterais
for (i=[0:2]){
translate([22.5,285,pe+esp_comp+i*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);
translate([22.5+660+45,285,pe+esp_comp+i*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);
}

translate([22.5,285,pe+esp_comp+h_gaveta+3*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);
translate([22.5+660+45,285,pe+esp_comp+h_gaveta+3*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);


translate([22.5,285,pe+esp_comp+h_gaveta+4*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);
translate([22.5+660+45,285,pe+esp_comp+h_gaveta+4*nicho+(nicho-esp_comp/2)/2])
rotate([90,0,0])
cylinder(270,15,15);


// Otimizações

//for (i=[0:4]){
//    if (i<3){
//        translate([30,0,pe+250*i])
//    } else {
//        translate([30,0,pe+250*i+h_gaveta])
//    }
//cube([690,300,15]);
//}
#+end_src

Note that the model is incomplete. I did not model the drawers completely, as I wanted to nail the dimentions and proportions in order to produce a bill of materials that I could purchase and transport easily. As a caveat, I was not able to retrieve the final quotes from the 3D model. This is something I am willing to explore on another post if I find a solution. The ability to export the final model dimensions is a must for any work on the shop and bring this project to life.
** DONE Add ssh-key to Keychain for Session Authentication :keychain:ssh:key:
CLOSED: [2022-06-15 Wed 18:35]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-06-15-add-ssh-key-to-keychain-for-session-authentication
:END:

Since github retired hhtp authentication, I have moved to the only option available: ssh-key authentication. It makes things easier by not having to type user and password multiple times in the same session. In theory. In practice, this was not my experience. This is when i found out about [[https://www.funtoo.org/Funtoo:Keychain][keychain]]. From their docs:

#+begin_quote
Keychain helps you to manage SSH and GPG keys in a convenient and secure manner. It acts as a frontend to ssh-agent and ssh-add, but allows you to easily have one long running ssh-agent process per system, rather than the norm of one ssh-agent per login session.

This dramatically reduces the number of times you need to enter your passphrase. With keychain, you only need to enter a passphrase once every time your local machine is rebooted.
#+end_quote

And how exactly do you add a ssh-key to the keychain? Here's how:

#+begin_src bash
guilherme@gtpedrosa:~$eval $(keychain --eval mysshkey)

 * keychain 2.8.5 ~ http://www.funtoo.org
 * Found existing ssh-agent: 1494
 * Adding 1 ssh key(s): /home/guilherme/.ssh/mysshkey
Enter passphrase for /home/guilherme/.ssh/mysshkey:
 * ssh-add: Identities added: /home/guilherme/.ssh/mysshkey
#+end_src

No need to insert usernames os passwords until you reboot.
** DONE Open Source Tools to Record a Conference Poster Presentation :oss:video:
CLOSED: [2022-06-26 Sun 22:00]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-06-24-open-source-tools-to-record-a-conference-poster-presentation
:END:

Recently, I and a colleague had a poster accepted to a conference we wanted to attend. Yay! However, we also had to send a video with the poster pitch along with the poster itself. And there, my friend, was an unanticipated challenge!

First of all, we found out about the video close to the deadline. Our fault. Then, it was difficult and confusing to figure out the etiquette of the whole thing. Should we go over the poster and zoom in and out to better present the content? Or should we make an accompanying slide deck for the pitch? We chose the slide deck approach, only to find out most presenters chose the poster one. We did not regret our decision, though. This is just a simple example of something we had for given that just wasn't.

And then, how to record? Without experience in the /métier/, I ended up with the following recording setup:

- [[https://obsproject.com/pt-br/kb/linux-installation][OBS - Open Broadcaster Software]]: Recording the screen, capturing video and audio.
- [[https://shotcut.org/][Shotcut]]: Video edition.

I recorded smaller sections without mistakes with OBS and stitched them on Shotcut. I also removed long pauses as I pleased to reduce the presentation length.

The result was pleasing. The video came out with good quality and a small file size. The learning curve of the software was none since both were pretty intuitive for the tasks at hand. I would 10/10 recommend the combo to anyone in a similar situation.
** DONE Updating HuC firmare on HP 246 G6 running Debian :linux:debian:
CLOSED: [2022-07-10 Sun 12:38]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-07-07-updating-huc-firmare-on-hp-246-g6-running-debian
:END:

After noticing a warning on my boot screen about missing firmware I tried to update the system. In the process, I have received messages similar to the one below:

#+begin_src bash
W: Possible missing firmware /lib/firmware/i915/bxt_dmc_ver1_07.bin for module i915
W: Possible missing firmware /lib/firmware/i915/skl_dmc_ver1_27.bin for module i915
#+end_src

These two resources were really helpful to me:

- [[https://blogs.igalia.com/vjaquez/2017/12/07/enabling-huc-for-sklkbl-in-debiantesting/][vjaquez post]]
- [[https://unix.stackexchange.com/questions/556946/possible-missing-firmware-lib-firmware-i915-for-module-i915][Unix Stack Exchange Thread]]

From the first resource, I have found what the heck is HuC:

#+begin_quote
HuC is a firmware, loaded by i915 kernel module, designed to offload some of the media functions from the CPU to GPU. One of these functions is bitrate control when encoding. HuC saves unnecessary CPU-GPU synchronization.
#+end_quote

Which puts things into perspective, even though I haven't noticed any graphics issue besides a Matlab running on OpenGL warning.

As a solution, the stackexchange post taught me about apt-file, an application that indexes and searches in my available repositories for a particular file. After its installation, I tried to find out the offending drivers:

#+begin_src bash :exports both :results output
apt-file search skl_huc
#+end_src

And install the package /firmware-linux/:

#+begin_src bash
sudo apt install firmware-linux
#+end_src

After this, the update went smoothly and the error messages on boot vanished.
** DONE On taking notes and learning methods :learning:anki:zettlekasten:
CLOSED: [2022-08-01 Mon 19:02]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-08-01-on-taking-notes-and-learning-methods
:END:

After finishing the book "How to take smart notes" by Sönke Ahrens, I came to realize the following: I have been exposed to two ways to learn that use notes as a basis of the system. The first is embodied by the Zettlekasten and depicted through Ahrens' book. The second relies on spaced repetition and /Ankification/ of notes if you will.

It is only natural to question myself which way (if any at all) I have found best suited for me. I do not have an answer yet and do not know if I ever will, even after having tried both methods, admittedly to different extents. However, I have stumbled upon two texts that might shed some light on what learning is for me and what method I believe might help me to reach there.

The first text is from [[https://www2.math.upenn.edu/~wilf/website/dek.pdf][Hebert Wilf on the occasion of Donald Knuth's 64th birthday]]. On the occasion, Knuth wrote Wilf a letter while on a plane, without access to any resources. The resulting 4 pages generated the following reaction on Wilf:

#+begin_quote
There followed four pages of tightly handwritten information that was a gold mine of the history of the sequence b(n) and related matters. It contained references to de Rham, Dijkstra, Carlitz, Neil Sloane, Stern (of Stern-Brocot trees fame), Eisenstein, Conway, Zagier, Sch ̈onhage, Hardy, Ramanujan, and so forth. It did not merely mention the names. It contained precise statements of the results of interest, of their relationship with the result of Calkin and myself and with each other. It contained a number of Don’s own observations about related questions, and had several conjectures, with background, to offer.
#+end_quote

What struck me is the breadth and depth of Knuth's knowledge, and how accessible it is to him. There was no need for a /second brain/ whatsoever. Pure interest, mingled with facts resulting in insights into the questions posed by Wilf and Calkin. This is to me a picture of what knowledge should look like and how it should operate. And by this, I do not mean we should all aim to be of Knuth's caliber. But that knowledge walks hand in hand with memory. Selection of what we memorize and retrieval is, indeed, important. And I, would not like to delegate it entirely to a second electronic brain.

The second text is from the introduction to Hannah Arendt's "Between past and future". The 9th edition presentation, written by José Carvalho, tries to elucidate why 'the present' was not directly referred to in the title. One of the passages by Arendt, which I'll translate freely:

#+begin_quote
For memory and depth are the same, or rather depth can only be reached by man through remembrance.
#+end_quote

Resonated with me deeply. It seems that delegating part of yourself to a tool to harness some efficiency improvements over productivity or creativity as Zettlekasten proposes comes at a cost of one's depth. We keep ourselves shallow so indexing and retrieval are optimized elsewhere. It seems a new version of Taylorism where instead of the division of labor between employees to complete assignments as efficiently as possible, one seeks the division of cognitive labor between yourself and a tool.

So, as a result, I believe I aim for depth. I am not optimizing for a productivity metric of some sort. I also do not claim I will not use the Zettlekasten, as I see it fit for academic applications. But as a general principle, I am more inclined to use SRS soon and see how I feel about the subjects studied. For now, this is all subjective and that is ok. Better explore the concepts and stick to something I find coherent given a north.

** DONE Using moderncv on Debian Fresh Installation :linux:debian:latex:
CLOSED: [2022-08-22 Mon 23:05]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-08-22-using-moderncv-on-debian-fresh-installation
:END:

The easiest way to install a minimal LaTeX installation to use moderncv was using the [[https://packages.debian.org/search?keywords=texlive][debian package repository]]:

#+begin_src bash
sudo apt install texlive
sudo apt install texlive-lang-portuguese
#+end_src

Note to self: I tried to install the vanilla texlive systemwide without the full installation (~7Gb) by using the graphical capability if /install-tl/ but could not get tlmgr to work properly, even after adding the installation path to my /.bashrc/.

** DONE Offline python documentation with Zeal and doc2dash :linux:python:documentation:solar:pvlib:
CLOSED: [2022-09-13 Tue 23:35]
:PROPERTIES:
:EXPORT_FILE_NAME: 2022-09-13-offline-python-documentation-with-zeal-and-doc2dash
:END:

While working on a python project today, the internet connection went down. For a while, I couldn't reference some API documentation online. This got me thinking: could I have them all offline? Later on (with connection!), I have found the answer: an offline documentation tool called Zeal.

If the project you are working on is popular, there are already docsets available for download. This will make the documentation for a given package accessible in the blink of an eye! However (and more likely) docsets won't be available. One alternative is to use doc2dash to generate them from the project documentation.

Using the excellent pvlib package as an example:

#+begin_src bash
pip install pvlib[doc]
pip install doc2dash
git clone https://github.com/pvlib/pvlib-python.git
cd pvlib-python/docs/sphinx
make html
doc2dash -A ./build/html
#+end_src

In Windows, I had some trouble with the *-A* flag which led me to manually move the docset to Zeal. Aside from this, I was pretty satisfied with the result and how fast I could reach the API reference without the internet.

** DONE Reading PVSyst's Module (PAN) and Inverter (OND) files with Python :python:pvlib:solar:
CLOSED: [2023-01-10 Tue 04:22]
:PROPERTIES:
:EXPORT_FILE_NAME: 2023-01-10-reading-pvsyst-s-module-pan-and-inverter-ond-files-with-python
:END:

Before diving into solar simulations with [[https://pvlib-python.readthedocs.io/en/stable/][PVLib]], one of the main concerns I had was how to use the same inputs given to PVSyst, the /de facto/ standard software of the solar industry. That also meant using the same files that describe the main equipments: **PAN** files for the modules and **OND** files for the inverters.

Although It surprised me that PVLib did not come with a PVSyst file's parser, It was not a deal breaker. The search for such a parser led me to [[https://github.com/frivollier/pvsyst_tools][pvsyst_tools]]. The library worked perfectly for the PAN file I had at hand. Here's a quick demo loading the library and PAN file from a local folder:

#+begin_src python :session :results value
import os, sys

spath = '~/Projects/solar/'
print('current workign directory: {}'.format(spath))
sys.path.append(os.path.join(spath,'libs'))

import pvsyst
print('pvsyst module path: {}'.format(pvsyst.__file__))

import logging
logger = logging.getLogger('pysyst')
logger.setLevel(10)  # 5 for Verbose 10 for Debug

pan = os.path.join(spath,'refs','CS6W-545MB-AG_CSI_EXT_1500V_V7_10_20210419.PAN')  # example PAN file

# parse .PAN file into dict
module_parameters = pvsyst.pan_to_module_param(pan)  # return two dicts

module_parameters
#+end_src

#+begin_src
{'Manufacturer': 'CSI Solar Co., Ltd.',
 'Model': 'CS6W-545MB-AG 1500V',
 'Technol': 'mtSiMono',
 'DataSource': 'Manufacture 2020 TUV-SUD data',
 'YearBeg': '2020',
 'Comment': 'www.csisolar.com',
 'Width': 1.134,
 'Height': 2.266,
 'Depth': 0.035,
 'Weight': 32.2,
 'RelEffic800': 0.27,
 'RelEffic400': -0.21,
 'RelEffic200': -1.8,
 'NCelS': 72,
 'NCelP': 2,
 'NDiode': 3,
 'GRef': 1000.0,
 'TRef': 25.0,
 'PNom': 545.0,
 'PNomTolLow': '0.00',
 'PNomTolUp': '1.80',
 'Isc': 13.95,
 'Voc': 49.4,
 'Imp': 13.14,
 'Vmp': 41.5,
 'muISC': 6.98,
 'muVocSpec': -128.4,
 'mIsc_percent': 0.05003584229390681,
 'mVoc_percent': -0.259919028340081,
 'muPmpReq': -0.34,
 'RShunt': 2500.0,
 'Rp_0': 10000.0,
 'Rp_Exp': 5.5,
 'RShunt_stc': 2530.6507857884803,
 'RSerie': 0.206,
 'Gamma': 0.98,
 'muGamma': -0.0004,
 'REM_Str_1': 'Frame: Anodized aluminium alloy',
 'REM_Str_2': 'Structure: 2.0mm Glass / EVA / 2.0mm Glass',
 'REM_Str_3': 'Connections: Cable,T4 series or H4 UTX or MC4-EVO2',
 'IAM_Point_1': '20.0,1.00000',
 'IAM_Point_2': '40.0,1.00000',
 'IAM_Point_3': '60.0,1.00000',
 'IAM_Point_4': '65.0,0.99000',
 'IAM_Point_5': '70.0,0.96000',
 'IAM_Point_6': '75.0,0.92000',
 'IAM_Point_7': '80.0,0.84000',
 'IAM_Point_8': '85.0,0.72000',
 'IAM_Point_9': '90.0,0.00000',
 'IAM': array([[20.  , 40.  , 60.  , 65.  , 70.  , 75.  , 80.  , 85.  , 90.  ],
        [ 1.  ,  1.  ,  1.  ,  0.99,  0.96,  0.92,  0.84,  0.72,  0.  ]]),
 'OperPoint_Point_1': 'False,800,25.0,0.27,0.00,0.000,0.000,0.00',
 'OperPoint_Point_2': 'False,600,25.0,0.14,0.00,0.000,0.000,0.00',
 'OperPoint_Point_3': 'False,400,25.0,-0.21,0.00,0.000,0.000,0.00',
 'OperPoint_Point_4': 'False,200,25.0,-1.80,0.00,0.000,0.000,0.00',
 'I_o_ref': 2.0352320417866327e-11,
 'EgRef': 1.121,
 'manufacturer': 'CSI Solar Co., Ltd.',
 'module_name': 'CS6W-545MB-AG 1500V',
 'Pmpp': 545.0,
 'Impp': 13.14,
 'Vmpp': 41.5,
 'mIsc': 0.00698,
 'mVocSpec': -0.12840000000000001,
 'mPmpp': -0.34,
 'Rshunt': 2500.0,
 'Rsh 0': 10000.0,
 'Rshexp': 5.5,
 'Rserie': 0.206,
 'gamma_ref': 0.98,
 'mu_gamma': -0.0004,
 'I_L_ref': 13.95,
 'R_sh_ref': 2530.6507857884803,
 'R_sh_0': 10000.0,
 'R_s': 0.206,
 'R_sh_exp': 5.5,
 'cells_in_series': 72,
 'alpha_sc': 0.00698}
#+end_src

To parse the OND file, however, I had to patch the ~inverter.py~ file from the /pvsyst_tools/ library to include an extra section called ~ProfilPIO~. That is why I decided to load the library form a local copy in the previous example. Here's the patch:

#+begin_src python
ond_sections ={'PVObject_': 'pvGInverter',
               'PVObject_Commercial': 'pvCommercial',
               'Converter': 'TConverter',
               'Remarks, Count': 'Remarks',
               'ProfilPIO': 'ProfilPIO',
               'ProfilPIOV1': 'ProfilPIOV1',
               'ProfilPIOV2': 'ProfilPIOV2',
               'ProfilPIOV3': 'ProfilPIOV3'}
#+end_src

With this patch in place, the same logic and procedure was applied to retrieve the inverter parameters:

#+begin_src python :session :results value
ond_dir = r'refs'  # directory of OND files}
ond = os.path.join(spath,ond_dir,'Sungrow_SG3125HV-30_V50_20200903_PVsyst.6.6.7.OND')  # example PAN file

# parse .OND file into dict
inverter_parameters = pvsyst.ond_to_inverter_param(ond)  # return two dicts

inverter_parameters
#+end_src

#+begin_src
{'pvGInverter': {'Comment': 'Sungrow\tSG3125HV-30\tManufacturer 2020',
  'Version': '6.67',
  'ParObj1': '2020',
  'Flags': '$003C1463',
  'pvCommercial': {'Comment': 'Sungrow Power Supply Co., Ltd',
   'Flags': '$0041',
   'Manufacturer': 'Sungrow',
   'Model': 'SG3125HV-30',
   'DataSource': 'Manufacturer 2020',
   'YearBeg': '2020',
   'Width': '2.250',
   'Height': '2.350',
   'Depth': '1.160',
   'Weight': '2700.00',
   'NPieces': '100',
   'PriceDate': '26/11/15 14:00',
   'Currency': 'EUR'},
  'Transfo': 'Without',
  'TConverter': {'PNomConv': '3125.000',
   'PMaxOUT': '3437.000',
   'VOutConv': '600.0',
   'VMppMin': '875',
   'VMPPMax': '1300',
   'VmppNom': '1100.0',
   'VAbsMax': '1500',
   'PSeuil': '3125.0',
   'EfficMax': '99.00',
   'EfficEuro': '98.70',
   'FResNorm': '3.00',
   'ModeOper': 'MPPT',
   'CompPMax': 'Lim',
   'CompVMax': 'Lim',
   'MonoTri': 'Tri',
   'ModeAffEnum': 'Efficiencyf_POut',
   'UnitAffEnum': 'kW',
   'IDCMax': '0.0',
   'INomAC': '3007.0',
   'IMaxAC': '3308.0',
   'TPNom': '50.0',
   'TPMax': '45.0',
   'TPLim1': '52.0',
   'TPLimAbs': '60.0',
   'PLim1': '2500.000',
   'PInEffMax ': '952507.900',
   'PThreshEff': '5673.4',
   'HasdefaultPThresh': 'False',
   'ProfilPIO': {'NPtsMax': '11',
    'NPtsEff': '8',
    'LastCompile': '$8089',
    'Mode': '1',
    'Point_1': '12500.0,0.0',
    'Point_2': '75000.0,69249.5',
    'Point_3': '125000.0,119172.4',
    'Point_4': '250000.0,243901.1',
    'Point_5': '500000.0,493020.8',
    'Point_6': '750000.0,741690.6',
    'Point_7': '1500000.0,1485000.0',
    'Point_8': '2500000.0,2469779.2',
    'Point_9': '0.0,0.0',
    'Point_10': '0.0,0.0',
    'Point_11': '0.0,0.0'},
   'VNomEff': '875.0,1100.0,1300.0,',
   'EfficMaxV': '99.030,98.860,98.690,',
   'EfficEuroV': '98.810,98.627,98.421,',
   'ProfilPIOV1': {'NPtsMax': '11',
    'NPtsEff': '9',
    'LastCompile': '$8089',
    'Mode': '1',
    'Point_1': '3125.0,0.0',
    'Point_2': '163952.7,159280.0',
    'Point_3': '320420.6,315390.0',
    'Point_4': '635380.4,628010.0',
    'Point_5': '792268.9,783950.0',
    'Point_6': '949459.8,940250.0',
    'Point_7': '1580566.0,1563970.0',
    'Point_8': '2370556.7,2342110.1',
    'Point_9': '3155956.3,3115560.1',
    'Point_10': '0.0,0.0',
    'Point_11': '0.0,0.0'},
   'ProfilPIOV2': {'NPtsMax': '11',
    'NPtsEff': '9',
    'LastCompile': '$8089',
    'Mode': '1',
    'Point_1': '3125.0,0.0',
    'Point_2': '164523.1,158880.0',
    'Point_3': '321647.8,315440.0',
    'Point_4': '636356.2,627320.0',
    'Point_5': '793900.1,783500.0',
    'Point_6': '950465.2,939630.0',
    'Point_7': '1582454.6,1563939.9',
    'Point_8': '2372756.9,2342860.1',
    'Point_9': '3170945.7,3124649.9',
    'Point_10': '0.0,0.0',
    'Point_11': '0.0,0.0'},
   'ProfilPIOV3': {'NPtsMax': '11',
    'NPtsEff': '9',
    'LastCompile': '$8089',
    'Mode': '1',
    'Point_1': '3125.0,0.0',
    'Point_2': '165697.0,158920.0',
    'Point_3': '323254.1,315690.0',
    'Point_4': '637521.6,627640.0',
    'Point_5': '795473.8,783860.0',
    'Point_6': '952507.9,940030.0',
    'Point_7': '1585300.1,1563740.0',
    'Point_8': '2376773.2,2342310.1',
    'Point_9': '3177379.0,3125270.0',
    'Point_10': '0.0,0.0',
    'Point_11': '0.0,0.0'}},
  'NbInputs': '18',
  'NbMPPT': '2',
  'TanPhiMin': '-0.750',
  'TanPhiMax': '0.750',
  'NbMSInterne': '2',
  'MasterSlave': 'No_M_S',
  'IsolSurvey ': 'Yes',
  'DC_Switch': 'Yes',
  'AC_Switch': 'Yes',
  'DiscAdjust': 'Yes',
  'MS_Thresh': '0.8',
  'Night_Loss': '120.00'}}
#+end_src

How to use the resulting dictionaries with the inverter and module parameters is a different story, better left for another post.

** DONE Getting started with Neovim: Notes to self :neovim:python:dofiles:
CLOSED: [2023-01-22 Sun 21:36]
:PROPERTIES:
:EXPORT_FILE_NAME: 2023-01-22-getting-started-with-neovim-notes-to-self
:END:

This week I wanted to up my python development `fu` by focusing on my tooling. My main criterium was to move fast between definitions and files to understand a new code base. For a while, I have been learning about vim from the ThePrimeagen Youtube channel. His latest video on [[https://www.youtube.com/watch?v=w7i4amO_zaE&t=817s][getting started with Neovim]] is simply an excellent resource.

So I took the time to implement each step following the video. In the process, not only I have learned a great deal about how to set up Neovim with Lua, but also what it is like to be /fluent in vim/, and I liked it. My new setup, which is a simplified version of the one in the video is now in [[https://github.com/gtpedrosa/dotfiles][dot files repository]].

Some of the shortcuts that I do not want to forget (not categorized):

- gd: Jump to definition
- Ctrl-o: Returns cursor to the previous position (e.g. before the jump to definition)
- '': Similar to Ctrl-o but returns to the line (not position)
- K: Shows help for the object under the cursor
- pf: Project find, works with files
- ps: Project search, works with code/text
- pv: Opens Netrw navigator
- f: finds a character in the current line after the cursor
- a: Inserts after the cursor
- J: Brings line below to current line
- zz: Centralize the cursor  in the middle of the screen
- Ctrl+e: Opens harpoon dialog
- Spc+a: Marks file to harpoon
- Ctrl+h: Switch to harpoon position 1
- Ctrl+t: Switch to harpoon position 2

  At this time, I still have not replaced my development workflow, with Jupyter-lab, with the Neovim setup. Since I work mainly with data, it is necessary to find the best way to bend the new tooling to my REPL workflow. However, getting acquainted with the new code base has been a delight with the new tooling so far.
** DONE Python development workflow with Neovim and Jupyter-lab :neovim:python:jupyter:
CLOSED: [2023-02-05 Sun 07:49]
:PROPERTIES:
:EXPORT_FILE_NAME: 2023-01-28-python-development-workflow-with-neovim-and-jupyter-lab
:END:

I want to expand on the previous post [[https://gtpedrosa.github.io/blog/getting-started-with-neovim-notes-to-self/][Getting started with nvim]] touching upon the jupyter-lab integration. I am still unpleased with the current solution, but the following trick addresses a couple of issues.

Jupyter-lab comes with an IPython interpreter, which in turn has an [[https://ipython.org/ipython-doc/3/config/extensions/autoreload.html][auto-reload extension]]. It can be enabled easily as:

#+begin_src
%load_ext autoreload
%autoreload 2
#+end_src

The magic command set to "2" reloads every module before any code is executed. In practice, this allows code edition in Neovim and REPL-like execution in jupyter-lab. Best of both worlds.

However, there are caveats. Pain points to solve include copy-pasting between editor and interpreter, and modifications to allow some of these snippets to be run as scripts. This could probably be solved using the [[https://github.com/jupyter-vim/jupyter-vim][jupyter-vim package]]. Unfortunately, I could not make it work with my project`s virtual environment.

The next steps involve translating the following vim script to lua:


#+begin_src
" Always use the same virtualenv for vim, regardless of what Python
" environment is loaded in the shell from which vim is launched
let g:vim_virtualenv_path = '/path/to/my/new/vim_virtualenv'
if exists('g:vim_virtualenv_path')
    pythonx import os; import vim
    pythonx activate_this = os.path.join(vim.eval('g:vim_virtualenv_path'), 'bin/activate_this.py')
    pythonx with open(activate_this) as f: exec(f.read(), {'__file__': activate_this})
endif
#+end_src

and including "+y" as "Y" remap in /remap.lua/ file just in case.

So far, the trade-offs have had a net positive impact.
** DONE Show specific labels in large plots in Excel :excel:plots:
CLOSED: [2023-02-21 Tue 08:12]
:PROPERTIES:
:EXPORT_FILE_NAME: 2023-02-21-show-specific-labels-in-large-plots-in-excel
:END:

Suppose you need to highlight specific labels in a large bar or column chart in Excel. Here is a two-step way to accomplish the task.

With the COUNT.IF formula, you can detect if the current label belongs to a list of specific ones. The neat trick, however, is in the number formatting used to show the labels of interest exclusively:

#+begin_src
[=0]"";0%
#+end_src

The formatting makes the cell show empty when zero (default output of the COUNT.IF) and a percentage otherwise (COUNT.IF*CELL output).


* Footnotes
* COMMENT Local Variables                          :ARCHIVE:
# Local Variables:
# eval: (org-hugo-auto-export-mode)
# End:
